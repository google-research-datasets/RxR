# RxR Landmarks and Model-Generated Instructions


### Reference

This data accompanies the following publications:

-   [Less is More: Generating Grounded Navigation Instructions from Landmarks](https://arxiv.org/abs/2111.12872)
    (CVPR 2022), and
-   [A New Path: Scaling Vision-and-Language Navigation with Synthetic
    Instructions and Imitation Learning](https://arxiv.org/abs/2210.03112) (CVPR
    2023).

Available downloads include:

-   **Silver Landmark Data**: 1m landmarks mentioned in RxR instructions that
    have been grounded in Matterport3D environments
-   **Marky-Matterport Instructions**: 1m navigation instructions generated by
    the Marky-mT5 model in Matterport3D environments
-   **Marky-Gibson Instructions**: 3.2m instructions generated by the Marky-mt5
    model in Gibson environmnents
-   **Gibson Navigation Graphs**: Navigation graphs to support agent movement
    and path sampling between panoramas in Gibson

For full details please refer to the papers.

Bibtex:

```
@inproceedings{marky-landmark,
  title={Less is More: Generating Grounded Navigation Instructions from Landmarks},
  author={Su Wang and Ceslee Montgomery and Jordi Orbay and Vighnesh Birodkar and Aleksandra Faust and Izzeddin Gur and Natasha Jaques and Austin Waters and Jason Baldridge and Peter Anderson},
  booktitle={Computer Vision and Pattern Recognition (CVPR)},
  year={2022},
}

@inproceedings{marky-landmark,
  title={A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning},
  author={Aishwarya Kamath* and Peter Anderson* and Su Wang and Jing Yu Koh and Alexander Ku and Austin Waters and Yinfei Yang and Jason Baldridge and Zarana Parekh},
  booktitle={Computer Vision and Pattern Recognition (CVPR)},
  year={2023},
}
```

### Download Links

The data is released as gzipped [JSON Lines](https://jsonlines.org/), and can be
downloaded from the links below.


#### Silver Landmark Data

Using text parsers, weak supervision from RxR's pose traces, and a multilingual
image-text encoder trained on 1.8b images, we identify 971k English, Hindi and
Telugu landmark descriptions in RxR instructions and ground them to specific
regions in panoramas from the
[Matterport3D](https://niessner.github.io/Matterport/) dataset.

*   [`rxr_landmarks_train_guide` (63M)](https://storage.cloud.google.com/rxr-data/rxr_landmarks_train_guide.jsonl.gz)
*   [`rxr_landmarks_val_seen_guide` (7M)](https://storage.cloud.google.com/rxr-data/rxr_landmarks_val_seen_guide.jsonl.gz)
*   [`rxr_landmarks_val_unseen_guide` (10M)](https://storage.cloud.google.com/rxr-data/rxr_landmarks_val_unseen_guide.jsonl.gz)

#### Marky-Matterport Instructions

We sample ~333k paths in [Matterport3D](https://niessner.github.io/Matterport/)
environments and annotate them with high-quality navigation instructions
generated by the Marky-mT5 model in English, Hindi and Telugu (~1m instructions
in total).

*   [`rxr_marky_train_guide` (255M)](https://storage.cloud.google.com/rxr-data/rxr_marky_train_guide.jsonl.gz)

#### Marky-Gibson Instructions

We sample ~1m paths in [Gibson](http://gibsonenv.stanford.edu/database/)
environments and annotate them with high-quality navigation instructions
generated by the Marky-mT5 model in English, Hindi and Telugu (~3.2m
instructions in total).

*   [`rxr_marky_gibson_en_train_guide` (117M)](https://storage.cloud.google.com/rxr-data/rxr_marky_gibson_en_train_guide.jsonl.gz)
*   [`rxr_marky_gibson_hi_train_guide` (162M)](https://storage.cloud.google.com/rxr-data/rxr_marky_gibson_hi_train_guide.jsonl.gz)
*   [`rxr_marky_gibson_te_train_guide` (150M)](https://storage.cloud.google.com/rxr-data/rxr_marky_gibson_te_train_guide.jsonl.gz)

#### Gibson Navigation Graphs

We construct navigation graphs for 492 Gibson environments (similar to the
Matterport3D nav-graphs released with the R2R instruction dataset).

*   [`gibson_navgraphs` (2M)](https://storage.cloud.google.com/rxr-data/gibson_navgraphs.gz)

### Data Format

We use the same fields as the RxR data wherever possible.

#### Silver Landmark Data

Each line in the jsonl data is a data item containing information about a single
instruction annotation from the RxR guide data.

Field descriptions:

*   `language` (str) Language tag, `en`/`hi`/`te` for English/Hindi/Telugu.
*   `instruction_id` (int) Uniquely identifies the guide annotation.
*   `split` (str) The annotation split: `train`, `val_seen`, `val_unseen`.
*   `heading` (float) Agent’s initial heading.
*   `scan` (str) Uniquely identifies a scan in the Matterport3D environment.
*   `text_spans`: (List[str]) Landmark text spans extracted from the RxR guide
    instruction and grounded into the environment.
*   `text_spans_chars_start_indices` (List[int]) Start char indices of text
    spans in the RxR guide instruction.
*   `text_spans_chars_end_indices` (List[int]) End char indices of text spans in
    the RxR guide instruction.
*   `landmark_source_panos` (List[str]) List of panorama IDs, with one
    corresponding to each landmark.
*   `landmark_source_pano_indices` (List[int]) List of indices of the
    landmark_source_panos in the corresponding RxR guide path.
*   `landmark_bbox_coords`: (List[List[float]]) Bounding boxes that identify
    each landmark in an equirectangular pano centered on the Matterport3D y
    axis. Each bbox is a list of 4 floats [ymin, xmin, ymax, xmax], where each
    float is between 0-1, i.e. normalized coordinates.
*   `landmark_angle_coords` (List[List[float]]) Angle coordinates (matched with
    the bbox coords) – heading, pitch, hfov, vfov in radians. heading: from the
    Matterport3D y-axis, right is increasing. pitch: from the Matterport3D
    horizon (xy-plane), up is increasing. hfov: Horizontal field-of-view. vfov:
    Vertical field-of-view.
*   `instruction` (str) The navigation instruction, generated by Marky-mT5 using
    the silver landmarks. Note that this is *not* the RxR guide instruction and
    does not correspond to the `text_spans` field.

Sample data:

```python
{
  "language": "en",
  "instruction_id": 19930,
  "split": "train",
  "heading": 1.04,
  "scan": "1pXnuDYAj8r",
  "text_spans": ["laundry room", "hallway", "railing", "carpet",
                 "corridor", "office area", "hallway", "painting", ...],
  "text_span_chars_start_indices": [21, 57, 74, 90, 156, 201, ...],
  "text_span_chars_end_indices": [33, 64, 81, 96, 164, 212, 247, 265, ...],
  "landmark_source_panos": ["7d887c096fa64a81a02723f8c283ca16",
                          "7d887c096fa64a81a02723f8c283ca16",
                          "6a3fba49106e4a9fb40cd0bf47dd4d46", ...],
  "landmark_source_pano_indices": [0, 0, 1, 1, 2, 5, 6, 3, 6, 6, 9, 9, 10],
  "landmark_bbox_coords": [
    [0.3055555, 0.062690742, 0.80555558, 0.34575980],
    [0.3791666, 0.633478232, 0.79583334, 0.88623034],
    ...],
  "landmark_angle_coords": [
    [-1.85840737, -0.17453292, 1.57079637, 1.57079637],
    [1.63271260, -0.274889349, 1.30899691, 1.30899691],
    ...],
  "instruction": "You're starting in a laundry room, facing the
  railing. Walk out of this laundry room onto the wooden flooring.
  Turn right and go down the hallway toward the end of this hallway.
  ...",
}
```

#### Marky-mT5 Instructions

For the model-generated data, the data items are much simpler as we only
publish the Marky-generated instructions.

Field descriptions:

*   `language` (str) Language tag, `en`/`hi`/`te` for English/Hindi/Telugu.
*   `path_id` (int) Uniquely identifies a path sampled from the Matterport3D
    environment.
*   `instruction_id` (int) Uniquely identifies the instruction annotation.
*   `instruction` (str) The navigation instruction, generated by Marky-mT5 using
    landmarks predicted for the path.
*   `heading` (float) The initial heading in radians. Following R2R, the heading
    angle is zero facing the y-axis with z-up, and increases by turning right.
*   `scan` (str) Uniquely identifies a scan in the Matterport3D environment.
*   `path` (List[str]) A sequence of panoramic viewpoints along the path.

### Rendering Landmark Images

For compliance with the
[Matterport3D Terms of Use](http://kaldir.vc.in.tum.de/matterport/MP_TOS.pdf) we
do not provide images of the landmarks. However, landmark images can be rendered
by downloading the
[Matterport3D dataset](https://niessner.github.io/Matterport/) and using the
[Matterport3D simulator](https://github.com/peteanderson80/Matterport3DSimulator).

The steps are as follows:

-   Follow the installation/build instructions for the
    [Matterport3D simulator](https://github.com/peteanderson80/Matterport3DSimulator).
    We recommend using the provided Docker and opting for off-screen GPU
    rendering using [EGL](https://www.khronos.org/egl/) (`cmake
    -DEGL_RENDERING=ON ..`). Further, we recommend changing `DOWNSIZED_WIDTH`
    and `DOWNSIZED_HEIGHT` in
    [downsize_skybox.py](https://github.com/peteanderson80/Matterport3DSimulator/blob/589d091b111333f9e9f9d6cfd021b2eb68435925/scripts/downsize_skybox.py#L14-L15)
    script to 1024 before running this install script. This will improve the
    quality of the rendered landmarks.
-   Download the silver landmark data from the links above and save it in the
    Matterport3DSimulator directory, along with the provided
    `render_landmarks.py` script.
-   Launch the docker and run the render script, e.g.:

```
xhost +
nvidia-docker run -it -e LANG=C.UTF-8 -e DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix --mount type=bind,source=$MATTERPORT_DATA_DIR,target=/root/mount/Matterport3DSimulator/data/v1/scans,readonly --volume `pwd`:/root/mount/Matterport3DSimulator mattersim:9.2-devel-ubuntu18.04
cd /root/mount/Matterport3DSimulator
python3 render_landmarks.py
```

For further details explaining `MATTERPORT_DATA_DIR` etc., please refer to the
[Matterport3DSimulator README](https://github.com/peteanderson80/Matterport3DSimulator).
Note that we have added `-e LANG=C.UTF-8` to the docker launch options in order
to load the unicode landmark data correctly.

The rendered images will be stored in directories with the structure
`landmarks/<split>/<language>/<instruction_id>_<landmark_index>.png`. The size
of the images and other options can be configured directly in
`render_landmarks.py`. As an example of the output, in the `landmarks` directory
we include landmark images for a single instruction (corresponding to Figure 4
in the [paper](https://arxiv.org/abs/2111.12872)).
